segmentation_processor_node:
  ros__parameters:
    input_segment_topic: "/ultralytics/segmentation/objects_segment"
    output_bbx3d_topic: "/darknet_ros_3d/bounding_boxes"
    point_cloud_topic: "/points"
    debug_pointcloud_topic: "/segmentation_processor/debug_pointcloud"
    debug_markers_topic: "/segmentation_processor/debug_markers"
    # this software requires that the X-axes points directly to the scene.
    # Use camera frame (e.g., "oak_left_camera_optical_frame") if no robot base running
    # Use "odom" if robot base running but no Nav2/SLAM
    # Use "map" if using full navigation stack
    # If frame doesn't exist, will use point cloud frame directly
    working_frame: "oak_left_camera_optical_frame"
    mininum_detection_threshold: 0.1
    minimum_probability: 0.35
    # interested_classes: ["orange","traffic light","chair","bottle","person", "italian_biscotti", "smoothie", "mango_juice", "crisps", "water", "sandwich", "toastie", "veggie_pot", "wrap", "espresso", "cappuccino", "americano","cup","table","bottle"]
    # Include both spellings to avoid model label mismatches (COCO uses "tire", custom models may use "tyre")
    # Added "license_plate" for direct license plate detection (more reliable than estimation)
    interested_classes: ["person","truck", "car", "tire", "tyre", "license_plate"]
